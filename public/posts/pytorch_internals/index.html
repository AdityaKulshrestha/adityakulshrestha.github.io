<!doctype html>
<html lang="en">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Liste - http://localhost:1313/">
    <title>Pytorch Compile Internals | Aditya Kulshrestha&#39;s Blogs</title>
    <meta name="description" content="A minimal hugo theme focus on content">
    <meta property="og:title" content="Pytorch Compile Internals" />
<meta property="og:description" content="Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let&rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/posts/pytorch_internals/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2025-07-22T14:12:54+00:00" />
<meta property="article:modified_time" content="2025-07-22T14:12:54+00:00" />


    <meta itemprop="name" content="Pytorch Compile Internals">
<meta itemprop="description" content="Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let&rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them."><meta itemprop="datePublished" content="2025-07-22T14:12:54+00:00" />
<meta itemprop="dateModified" content="2025-07-22T14:12:54+00:00" />
<meta itemprop="wordCount" content="3180">
<meta itemprop="keywords" content="" />
    
    <link rel="canonical" href="http://localhost:1313/posts/pytorch_internals/">
    <link rel="icon" href="http://localhost:1313//assets/favicon.ico">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link href="https://www.google-analytics.com" rel="preconnect" crossorigin>
    <link rel="alternate" type="application/atom+xml" title="Aditya Kulshrestha&#39;s Blogs" href="http://localhost:1313//atom.xml" />
    <link rel="alternate" type="application/json" title="Aditya Kulshrestha&#39;s Blogs" href="http://localhost:1313//feed.json" />
    <link rel="shortcut icon" type="image/png" href="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAQAAAC1HAwCAAAAC0lEQVR42mNk+A8AAQUBAScY42YAAAAASUVORK5CYII=">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Bricolage+Grotesque">
    
    <style>*,:after,:before{box-sizing:border-box;padding:0}body{font:1rem/1.5 bricolage grotesque,-apple-system,BlinkMacSystemFont,segoe ui,Helvetica,Arial,sans-serif;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;padding:2rem;background:#fffdfa;color:#000}.skip-link{position:absolute;top:-40px;left:0;background:#eee;z-index:100}.skip-link:focus{top:0}header{display:flex;justify-content:space-between;align-items:center;position:relative;padding-bottom:1rem}header .burger{display:none;background:0 0;border:none;padding:0;cursor:pointer}header .burger span{display:block;width:25px;height:3px;background:#000;margin:5px 0;transition:all .3s ease}@media(max-width:768px){header .burger{display:block;z-index:2}header .nav-menu{position:fixed;top:0;right:-100%;width:100%;height:100vh;background:#fffdfa;padding:2rem;transition:.3s ease;z-index:1}header .nav-menu.active{right:0}header .nav-menu ul{flex-direction:column;align-items:center;width:100%}header .nav-menu ul li{display:block;font-size:1.2rem;border-bottom:1px dotted #000;height:50px;display:flex;justify-content:center;align-items:center}header.menu-open .burger span:first-child{transform:rotate(45deg)translate(5px,6px)}header.menu-open .burger span:nth-child(2){opacity:0}header.menu-open .burger span:last-child{transform:rotate(-45deg)translate(5px,-6px)}}header a{text-decoration:none}header ul{list-style-type:none;padding:0}header li,header a{display:inline}.link{overflow:hidden;text-overflow:ellipsis;white-space:nowrap;overflow:hidden;text-overflow:ellipsis;text-decoration:none}.time{font-variant-numeric:tabular-nums;white-space:nowrap}blockquote{border-left:5px solid #eee;padding-left:1rem;margin:0}a,a:visited{color:inherit}a:hover,a.heading-link{text-decoration:none}pre{padding:.5rem;overflow:auto;overflow-x:scroll;overflow-wrap:normal}code,pre{font-family:San Francisco Mono,Monaco,consolas,lucida console,dejavu sans mono,bitstream vera sans mono,monospace;font-size:normal;font-size:small;background:#eee}code{margin:.1rem;border:none;overflow:visible;overflow-wrap:anywhere}ul{list-style-type:square}ul,ol{padding-left:1.2rem}.list{line-height:2;list-style-type:none;padding-left:0}.list li{padding-bottom:.1rem}.meta{color:#777}.content{max-width:70ch;margin:0 auto}h2.post{padding-top:.5rem}header ul a:first-child{padding-left:1rem}.nav{height:1px;background:#000;content:'';max-width:10%}.list li{display:flex;align-items:baseline}.list li time{flex:initial}.hr-list{margin-top:0;margin-bottom:0;margin-right:.5rem;margin-left:.5rem;height:1px;border:0;border-bottom:1px dotted #ccc;flex:1 0 1rem}.m,hr{border:0;margin:3rem 0}img{max-width:100%;height:auto}.post-date{margin:5% 0}.index-date{color:#9a9a9a}.animate-blink{animation:opacity 1s infinite;opacity:1}@keyframes opacity{0%{opacity:1}50%{opacity:.5}100%{opacity:0}}.tags{display:flex;justify-content:space-between}.tags ul{padding:0;margin:0}.tags li{display:inline}.avatar{height:120px;width:120px;position:relative;margin:-10px 0 0 15px;float:right;border-radius:50%}table{width:100%;border-collapse:collapse}th,td{border:1px solid #ddd;text-align:left;padding:8px}th{background-color:#f2f2f2}</style>
  
    
    
    
    <script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "articleSection": "posts",
        "name": "Pytorch Compile Internals",
        "headline": "Pytorch Compile Internals",
        "alternativeHeadline": "",
        "description": "Ever wondered what goes inside when you actually call torch.compile(model)? There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called Intermediate Representation (IRs) (more about this later) to the next component in the sequence. Let\u0026rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them.",
        "inLanguage": "en-us",
        "isFamilyFriendly": "true",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "http:\/\/localhost:1313\/posts\/pytorch_internals\/"
        },
        "author" : {
            "@type": "Person",
            "name": ""
        },
        "creator" : {
            "@type": "Person",
            "name": ""
        },
        "accountablePerson" : {
            "@type": "Person",
            "name": ""
        },
        "copyrightHolder" : "Aditya Kulshrestha\u0027s Blogs",
        "copyrightYear" : "2025",
        "dateCreated": "2025-07-22T14:12:54.00Z",
        "datePublished": "2025-07-22T14:12:54.00Z",
        "dateModified": "2025-07-22T14:12:54.00Z",
        "publisher":{
            "@type":"Organization",
            "name": "Aditya Kulshrestha's Blogs",
            "url": "http://localhost:1313/",
            "logo": {
                "@type": "ImageObject",
                "url": "http:\/\/localhost:1313\/assets\/favicon.ico",
                "width":"32",
                "height":"32"
            }
        },
        "image": "http://localhost:1313/assets/favicon.ico",
        "url" : "http:\/\/localhost:1313\/posts\/pytorch_internals\/",
        "wordCount" : "3180",
        "genre" : [ ],
        "keywords" : [ ]
    }
    </script>
     

    <script>
        document.addEventListener('DOMContentLoaded', () => {
            const burger = document.querySelector('.burger');
            const nav = document.querySelector('.nav-menu');
            const header = document.querySelector('header');
        
            if (burger) {
                burger.addEventListener('click', () => {
                    nav.classList.toggle('active');
                    header.classList.toggle('menu-open');
                });
        
                
                document.addEventListener('click', (e) => {
                    if (!header.contains(e.target) && nav.classList.contains('active')) {
                        nav.classList.remove('active');
                        header.classList.remove('menu-open');
                    }
                });
            }
        });
    </script> 
  </head>

<body>
  <a class="skip-link" href="#main">Skip to main</a>
  <main id="main">
  <div class="content">
    
<header>
  <p style="padding: 0;margin: 0;">
    <a href="http://localhost:1313/">
      <b>Aditya Kulshrestha&#39;s Blogs</b>
      <span class="text-stone-500 animate-blink">▮</span>
    </a>
  </p>

  
  <ul style="padding: 0;margin: 0;">
    
    
    <li class="">
      <a href="/posts/"><span>Post</span></a>
      
    </li>
  </ul>
  
</header>
<hr class="hr-list" style="padding: 0;margin: 0;">
    <section>
      <h2 class="post">Pytorch Compile Internals</h2>
      <p>Ever wondered what goes inside when you actually call torch.compile(model)?
There are a bunch of individual components that work together in a sequential manner to squeeze out the best performance from the GPU. Each component passes on some intermediate packets called <em>Intermediate Representation (IRs)</em> (more about this later) to the next component in the sequence.
Let&rsquo;s take a look what are the these components at an overview level and then we will dive deep into each one of them.</p>
<p><em>PS - I got a little bonus at the end!</em></p>
<h3 id="torchcompile-components">torch.compile components</h3>
<ul>
<li><strong>TorchDynamo</strong>: The fronend responsible for intercepting the Python code and convert it to graphs.</li>
<li><strong>AOTAutograd</strong> - Takes care of automatic differentiation for backpropagation. Doesn&rsquo;t gets activate for inference only workloads.</li>
<li><strong>TorchInductor</strong> - Performs optimization including Fusion and converts the input FX Graph into triton code (for GPUs) or C++ code (for CPUs).</li>
</ul>
<div style="text-align: center;">
  <img src="https://media2.dev.to/dynamic/image/width=800%2Cheight=%2Cfit=scale-down%2Cgravity=auto%2Cformat=auto/https%3A%2F%2Fdev-to-uploads.s3.amazonaws.com%2Fuploads%2Farticles%2Fmiy3lxuoigpfupv5zogz.png"
       alt="Components Overview: Illustration taken from Minwook Je blog on torch.compile vs torch.export"
       style="width: 500px; max-width: 100%; height: auto;" />
  <div style="font-size: 0.95em; color: #666; margin-top: 4px;">
    Components Overview: Illustration taken from Minwook Je blog on torch.compile vs torch.export
  </div>
</div>
<h3 id="1-torchdynamo">1. TorchDynamo</h3>
<p>TorchDynamo is the first and foremost component in the sequence which is responsible for intercepting the Python code.
It intercepts the Python bytecode at runtime and rewrites blocks of user code into graphs. This involves extracting subgraphs containing PyTorch operations while leaving the non-PyTorcch code untouched.</p>
<p><em>Shape Polymorphism</em> - Shape polymorphism is the ability of function to adapt to dynamic shape changes. When you do torch.compile, TorchDynamo doesn&rsquo;t prepare the graph for a static shaped input tensors. But it uses symbolic representation of the tensors shape which allows it to accept dynamic shapes as well.</p>
<p>How it works?</p>
<ul>
<li>
<p><strong>Symbolic Shape Representation</strong> - During the graph capture, the dimension sizes are not kept fixed but are treated <em>symbolically</em>. This helps the graph to work with any dynamic shape. TorchDynamo uses SymPy symbolic math library to represent these unknown shapes. The symbolic shapes are passed through the IR enabling TorchInductor to generate the code that is valid for any runtime shape matching the symbolic pattern. It uses something similar to a faketensor (more precisely ShapeEnv attached to a FakeTensorMode) which keeps track of symbolic shape state.</p>
</li>
<li>
<p><strong>Guards</strong> - To ensure that the input shapes matches to that of final symbolic shape represented by the TorchInductor. It relies on something called as Guard which are responsible for guarding (allowing) only those tensors which matches the symbolic shape.
If any data is not matched, the guard triggers a recompilation for those shape and stores it for future reference.
The Guard checks the following torch.Tensor properties:</p>
<ul>
<li>Python class of the tensor (tensor subclassing, etc)</li>
<li>dtype</li>
<li>device</li>
<li>requires_grad</li>
<li>dispatch_key (with thread-local includes/excludes applied)</li>
<li>ndim</li>
<li>sizes*</li>
<li>strides*</li>
</ul>
</li>
<li>
<p><strong>Loop-Level IR and Indexing</strong> - The loops and memory access patterns are also expressed through symbolic representations so that  the output shapes, stride calculations and buffer allocations all adapt to the actual runtime of the input tensor shape.</p>
</li>
<li>
<p><strong>Meta Functions and Shape Propagation</strong> - Each PyTorch operator traces &ldquo;meta&rdquo; computation, meaning functions that can deduce the output shape for any arbitrary shape inputs without explicitly performing the tensor computations. It enables TorchInductor to carry information regarding unknown dimension through all computations and memory allocations.</p>
</li>
<li>
<p><strong>Efficient Reuse and Specialization</strong> - The compilation for one family of inputs shapes (one that fits the guard) can be reused. Only new inputs shape which doesn&rsquo;t fit the guards needs recompilation.</p>
</li>
</ul>
<p><strong>Fallback</strong> - If the code can&rsquo;t be converted into graphs, it safely fall backs to PyTorch&rsquo;s eager execution.</p>
<p><strong>Intermediate Representation (IR)</strong> - FX Graph and modified bytecode capturing only PyTorch operations and tensors and leaving normal Python non-essentials.</p>
<p>Let&rsquo;s take an example of a simple <em>Self Attention Code (without masking)</em>:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">Attention</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>  <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_q</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">bias</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_k</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">bias</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_v</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">bias</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">:</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">Tensor</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">q</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_q</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">k</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_k</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">v</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_v</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">out</span> <span style="color:#f92672">=</span> <span style="color:#111">q</span> <span style="color:#f92672">@</span> <span style="color:#111">k</span><span style="color:#f92672">.</span><span style="color:#111">transpose</span><span style="color:#111">(</span><span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#111">,</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Apply softmax manually</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">out</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">exp</span><span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span> <span style="color:#f92672">/</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">sum</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">exp</span><span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">),</span> <span style="color:#111">dim</span><span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">keepdim</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">return</span> <span style="color:#111">out</span> <span style="color:#f92672">@</span> <span style="color:#111">v</span>
</span></span></code></pre></div><p>To see the output logs, you need to call PyTorch internal logging api</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">torch</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">logging</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">torch._logging</span> <span style="color:#00a8c8">as</span> <span style="color:#111">logging_api</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> <span style="color:#111">torch._dynamo.config</span> <span style="color:#00a8c8">as</span> <span style="color:#111">dcfg</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Enable logs for torchdynamo and also show the generated bytecode</span>
</span></span><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">dynamo</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">INFO</span><span style="color:#111">,</span> <span style="color:#111">bytecode</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span> <span style="color:#75715e">#, graph=False, output_code=False, autograd=False, aot_graphs=False, inductor=False)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">dcfg</span><span style="color:#f92672">.</span><span style="color:#111">verbose</span> <span style="color:#f92672">=</span> <span style="color:#00a8c8">True</span>  <span style="color:#75715e"># adds extra Dynamo verbosity</span>
</span></span><span style="display:flex;"><span><span style="color:#111">dcfg</span><span style="color:#f92672">.</span><span style="color:#111">suppress_errors</span> <span style="color:#f92672">=</span> <span style="color:#00a8c8">False</span>
</span></span></code></pre></div><p>Let&rsquo;s run the torch.compile,</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">dynamo</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">INFO</span><span style="color:#111">,</span> <span style="color:#111">bytecode</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">Attention</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">64</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">to</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">)(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span> <span style="color:#75715e"># Resets the cache and graph</span>
</span></span></code></pre></div><p>Output</p>
<ul>
<li>Input Bytecode</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  <span style="color:#ae81ff">8</span>           <span style="color:#ae81ff">0</span> <span style="color:#111">RESUME</span>                   <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#ae81ff">9</span>           <span style="color:#ae81ff">2</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>              <span style="color:#ae81ff">4</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">W_q</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">26</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">1</span> <span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">28</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">32</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">42</span> <span style="color:#111">STORE_FAST</span>               <span style="color:#ae81ff">2</span> <span style="color:#111">(</span><span style="color:#111">q</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">10</span>          <span style="color:#ae81ff">44</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">46</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">1</span> <span style="color:#111">(</span><span style="color:#111">W_k</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">68</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">1</span> <span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">70</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">74</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">84</span> <span style="color:#111">STORE_FAST</span>               <span style="color:#ae81ff">3</span> <span style="color:#111">(</span><span style="color:#111">k</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">11</span>          <span style="color:#ae81ff">86</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">88</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">2</span> <span style="color:#111">(</span><span style="color:#111">W_v</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">110</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">1</span> <span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">112</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">116</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">126</span> <span style="color:#111">STORE_FAST</span>               <span style="color:#ae81ff">4</span> <span style="color:#111">(</span><span style="color:#111">v</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">13</span>         <span style="color:#ae81ff">128</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">2</span> <span style="color:#111">(</span><span style="color:#111">q</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">130</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">3</span> <span style="color:#111">(</span><span style="color:#111">k</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">132</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">3</span> <span style="color:#111">(</span><span style="color:#111">transpose</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">154</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">1</span> <span style="color:#111">(</span><span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">156</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">2</span> <span style="color:#111">(</span><span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">158</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">162</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">172</span> <span style="color:#111">BINARY_OP</span>                <span style="color:#ae81ff">4</span> <span style="color:#111">(</span><span style="color:#f92672">@</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">176</span> <span style="color:#111">STORE_FAST</span>               <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">16</span>         <span style="color:#ae81ff">178</span> <span style="color:#111">LOAD_GLOBAL</span>              <span style="color:#ae81ff">8</span> <span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">190</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">exp</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">212</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">214</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">218</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">228</span> <span style="color:#111">LOAD_GLOBAL</span>              <span style="color:#ae81ff">8</span> <span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">240</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">6</span> <span style="color:#111">(</span><span style="color:#111">sum</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">262</span> <span style="color:#111">LOAD_GLOBAL</span>              <span style="color:#ae81ff">8</span> <span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">274</span> <span style="color:#111">LOAD_METHOD</span>              <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">exp</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">296</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">298</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">302</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">312</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">2</span> <span style="color:#111">(</span><span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">314</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">3</span> <span style="color:#111">(</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">316</span> <span style="color:#111">KW_NAMES</span>                 <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">318</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">322</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">332</span> <span style="color:#111">BINARY_OP</span>               <span style="color:#ae81ff">11</span> <span style="color:#111">(</span><span style="color:#f92672">/</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">336</span> <span style="color:#111">STORE_FAST</span>               <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">17</span>         <span style="color:#ae81ff">338</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">340</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">4</span> <span style="color:#111">(</span><span style="color:#111">v</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">342</span> <span style="color:#111">BINARY_OP</span>                <span style="color:#ae81ff">4</span> <span style="color:#111">(</span><span style="color:#f92672">@</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">346</span> <span style="color:#111">RETURN_VALUE</span>
</span></span></code></pre></div><table>
<thead>
<tr>
<th>Column Index</th>
<th>Example</th>
<th>Meaning</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>0</code></td>
<td><code>13</code></td>
<td><strong>Source code line number</strong> (from the Python file)</td>
</tr>
<tr>
<td><code>1</code></td>
<td><code>128</code></td>
<td><strong>Bytecode offset (address)</strong> — tells where in memory this opcode lives (used for jumps, flow control)</td>
</tr>
<tr>
<td><code>2</code></td>
<td><code>LOAD_FAST</code></td>
<td><strong>Opcode (instruction name)</strong> — the operation being done</td>
</tr>
<tr>
<td><code>3</code></td>
<td><code>2</code></td>
<td><strong>Operand/Argument</strong> — here, it&rsquo;s the index of the local variable</td>
</tr>
<tr>
<td><code>4</code></td>
<td><code>(q)</code></td>
<td><strong>Resolved name</strong> (if known) — in this case, local variable <code>q</code></td>
</tr>
</tbody>
</table>
<ul>
<li>Compiled ouput ByteCode</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  <span style="color:#ae81ff">8</span>           <span style="color:#ae81ff">0</span> <span style="color:#111">RESUME</span>                   <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>              <span style="color:#ae81ff">2</span> <span style="color:#111">LOAD_GLOBAL</span>             <span style="color:#ae81ff">19</span> <span style="color:#111">(</span><span style="color:#111">NULL</span> <span style="color:#f92672">+</span> <span style="color:#111">__compiled_fn_3</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">14</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">16</span> <span style="color:#111">LOAD_ATTR</span>               <span style="color:#ae81ff">10</span> <span style="color:#111">(</span><span style="color:#111">_modules</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">26</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">5</span> <span style="color:#111">(</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">28</span> <span style="color:#111">BINARY_SUBSCR</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">38</span> <span style="color:#111">LOAD_ATTR</span>               <span style="color:#ae81ff">11</span> <span style="color:#111">(</span><span style="color:#111">_parameters</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">48</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">6</span> <span style="color:#111">(</span><span style="color:#d88200">&#39;weight&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">50</span> <span style="color:#111">BINARY_SUBSCR</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">60</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">1</span> <span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">62</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">64</span> <span style="color:#111">LOAD_ATTR</span>               <span style="color:#ae81ff">10</span> <span style="color:#111">(</span><span style="color:#111">_modules</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">74</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">7</span> <span style="color:#111">(</span><span style="color:#d88200">&#39;W_k&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">76</span> <span style="color:#111">BINARY_SUBSCR</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">86</span> <span style="color:#111">LOAD_ATTR</span>               <span style="color:#ae81ff">11</span> <span style="color:#111">(</span><span style="color:#111">_parameters</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">96</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">6</span> <span style="color:#111">(</span><span style="color:#d88200">&#39;weight&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>             <span style="color:#ae81ff">98</span> <span style="color:#111">BINARY_SUBSCR</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">108</span> <span style="color:#111">LOAD_FAST</span>                <span style="color:#ae81ff">0</span> <span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">110</span> <span style="color:#111">LOAD_ATTR</span>               <span style="color:#ae81ff">10</span> <span style="color:#111">(</span><span style="color:#111">_modules</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">120</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">8</span> <span style="color:#111">(</span><span style="color:#d88200">&#39;W_v&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">122</span> <span style="color:#111">BINARY_SUBSCR</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">132</span> <span style="color:#111">LOAD_ATTR</span>               <span style="color:#ae81ff">11</span> <span style="color:#111">(</span><span style="color:#111">_parameters</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">142</span> <span style="color:#111">LOAD_CONST</span>               <span style="color:#ae81ff">6</span> <span style="color:#111">(</span><span style="color:#d88200">&#39;weight&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">144</span> <span style="color:#111">BINARY_SUBSCR</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">154</span> <span style="color:#111">PRECALL</span>                  <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">158</span> <span style="color:#111">CALL</span>                     <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">168</span> <span style="color:#111">UNPACK_SEQUENCE</span>          <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>            <span style="color:#ae81ff">172</span> <span style="color:#111">RETURN_VALUE</span>
</span></span></code></pre></div><p>See the difference? The modified byte code only contains the important stuff from a PyTorch perspective and removed everything at the Python level.</p>
<ul>
<li>Guards</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">GUARDS</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">RootGuardManager</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> <span style="color:#111">DEFAULT_DEVICE</span><span style="color:#111">:</span> <span style="color:#111">utils_device</span><span style="color:#f92672">.</span><span style="color:#111">CURRENT_DEVICE</span> <span style="color:#f92672">==</span> <span style="color:#00a8c8">None</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> <span style="color:#111">GLOBAL_STATE</span><span style="color:#111">:</span> <span style="color:#111">___check_global_state</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> <span style="color:#111">TORCH_FUNCTION_MODE_STACK</span><span style="color:#111">:</span> <span style="color:#111">___check_torch_function_mode_stack</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;x&#39;</span><span style="color:#111">],</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;x&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> <span style="color:#111">TENSOR_MATCH</span><span style="color:#111">:</span> <span style="color:#111">check_tensor</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;x&#39;</span><span style="color:#111">],</span> <span style="color:#111">Tensor</span><span style="color:#111">,</span> <span style="color:#111">DispatchKeySet</span><span style="color:#111">(</span><span style="color:#111">CUDA</span><span style="color:#111">,</span> <span style="color:#111">BackendSelect</span><span style="color:#111">,</span> <span style="color:#111">ADInplaceOrView</span><span style="color:#111">,</span> <span style="color:#111">AutogradCUDA</span><span style="color:#111">),</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#111">requires_grad</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">,</span> <span style="color:#111">size</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">],</span> <span style="color:#111">stride</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#ae81ff">256</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> <span style="color:#111">NO_HASATTR</span><span style="color:#111">:</span> <span style="color:#111">hasattr</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;x&#39;</span><span style="color:#111">],</span> <span style="color:#d88200">&#39;_dynamo_dynamic_indices&#39;</span><span style="color:#111">)</span> <span style="color:#f92672">==</span> <span style="color:#00a8c8">False</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">],</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> <span style="color:#111">TYPE_MATCH</span><span style="color:#111">:</span> <span style="color:#111">___check_type_id</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">],</span> <span style="color:#ae81ff">539598912</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">__dict__</span><span style="color:#111">,</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">GetGenericDictGuardAccessor</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">,</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;_modules&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> <span style="color:#111">DICT_LENGTH</span><span style="color:#111">:</span> <span style="color:#111">len</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">)</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">],</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">-</span> <span style="color:#111">TYPE_MATCH</span><span style="color:#111">:</span> <span style="color:#111">___check_type_id</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">],</span> <span style="color:#ae81ff">519852416</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">__dict__</span><span style="color:#111">,</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">GetGenericDictGuardAccessor</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">-</span> <span style="color:#111">DICT_CONTAINS</span><span style="color:#111">:</span> <span style="color:#f92672">not</span> <span style="color:#111">___dict_contains</span><span style="color:#111">(</span><span style="color:#d88200">&#39;forward&#39;</span><span style="color:#111">,</span> <span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">__dict__</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_parameters</span><span style="color:#111">,</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;_parameters&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">-</span> <span style="color:#111">DICT_LENGTH</span><span style="color:#111">:</span> <span style="color:#111">len</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_parameters</span><span style="color:#111">)</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_parameters</span><span style="color:#111">[</span><span style="color:#d88200">&#39;weight&#39;</span><span style="color:#111">],</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;weight&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">-</span> <span style="color:#111">TENSOR_MATCH</span><span style="color:#111">:</span> <span style="color:#111">check_tensor</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_parameters</span><span style="color:#111">[</span><span style="color:#d88200">&#39;weight&#39;</span><span style="color:#111">],</span> <span style="color:#111">Parameter</span><span style="color:#111">,</span> <span style="color:#111">DispatchKeySet</span><span style="color:#111">(</span><span style="color:#111">CUDA</span><span style="color:#111">,</span> <span style="color:#111">BackendSelect</span><span style="color:#111">,</span> <span style="color:#111">ADInplaceOrView</span><span style="color:#111">,</span> <span style="color:#111">AutogradCUDA</span><span style="color:#111">),</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#111">requires_grad</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">,</span> <span style="color:#111">size</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">],</span> <span style="color:#111">stride</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">-</span> <span style="color:#111">GuardManager</span><span style="color:#111">:</span> <span style="color:#111">source</span><span style="color:#f92672">=</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_parameters</span><span style="color:#111">[</span><span style="color:#d88200">&#39;bias&#39;</span><span style="color:#111">],</span> <span style="color:#111">accessed_by</span><span style="color:#f92672">=</span><span style="color:#111">DictGetItemGuardAccessor</span><span style="color:#111">(</span><span style="color:#d88200">&#39;bias&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">-</span> <span style="color:#111">ID_MATCH</span><span style="color:#111">:</span> <span style="color:#111">___check_obj_id</span><span style="color:#111">(</span><span style="color:#111">L</span><span style="color:#111">[</span><span style="color:#d88200">&#39;self&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_modules</span><span style="color:#111">[</span><span style="color:#d88200">&#39;W_q&#39;</span><span style="color:#111">]</span><span style="color:#f92672">.</span><span style="color:#111">_parameters</span><span style="color:#111">[</span><span style="color:#d88200">&#39;bias&#39;</span><span style="color:#111">],</span> <span style="color:#ae81ff">9695488</span><span style="color:#111">)</span>
</span></span></code></pre></div><h3 id="2-ahead-of-time-autograd-aotautograd">2. Ahead-Of-Time Autograd (AOTAutograd)</h3>
<p>As the name suggests AOTAutograd is responsible for differentiation and backpropagation graph generation.
it generates the backware computation graph (needed for the gradients and training) from the captured forward graph (passed by the TorchDynamo). However it only captures the backward graph and doesn&rsquo;t apply the graph level optimization.</p>
<h3 id="3-torchinductor">3. TorchInductor</h3>
<p>It further optimizes the graph and generates code to finally run on the hardware. It takes a simplified computation graphs and generates hihgly optimized low level code for the target hardware (CPU, HPU, GPU).</p>
<p>It also determines hardware level optimizations such as memory planning, tiling etc.</p>
<p>Due to multiple hardware adoption, torchinductor supports multiple backend
based on the target hardware.</p>
<h4 id="backend-supported-in-torchinductor">Backend supported in TorchInductor</h4>
<table>
<thead>
<tr>
<th>Backend</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Inductor</td>
<td>Default backend: highly optimized for CPUs and GPUs</td>
</tr>
<tr>
<td>Eager</td>
<td>Runs the model without the graph capture, no optimization happens in this mode</td>
</tr>
<tr>
<td>aot_eager</td>
<td>It applies the AutoAutograd to capture the graph but doesn&rsquo;t apply any further backend optimization</td>
</tr>
<tr>
<td>cudagraphs</td>
<td>Leverages CUDA Graphs for reduces CPU overhead</td>
</tr>
<tr>
<td>ipex</td>
<td>Uses Intel Extension for PyTorch for CPU-optimized execution</td>
</tr>
<tr>
<td>onnxrt</td>
<td>Uses ONNX runtime for acceleration on CPU/GPU</td>
</tr>
<tr>
<td>torch_tensorrt</td>
<td>TensorRT-backend for high-speed inference on Nvidia-GPUs</td>
</tr>
<tr>
<td>tvm</td>
<td>Uses Apache TVM compiler for cross hardware inference</td>
</tr>
<tr>
<td>openvino</td>
<td>Uses Intel OpenVINO for accelerated inference on supported Intel hardware</td>
</tr>
</tbody>
</table>
<p><em>Note that depending on the type inference/training, the supported backends might change.</em></p>
<p>Let&rsquo;s consider the same Attention Block example and let&rsquo;s see what optimizations TorchInductor does</p>
<ul>
<li>
<p><strong>Fusion</strong></p>
<p>Fusion is basically nothing but merging mutiple operations in kernel into one. So rahter than following the loop of reading from memory -&gt; Performing operations -&gt; Writing back to memory for each operation. Fusion allows to sandwich all the operations into one layer so that it becomes; reading from memory -&gt; Perform all the operations at once -&gt; Write back to the memory. This saves us to and from time of reading and writing from and to memory.</p>
<p>Assume that the geometrical shapes are the data points which are juggling between the memory and compute (your GPU). Now after every compute you send the data points back to the memory. This takes up a lot of time.</p>
  <div style="text-align: center;">
  <img src="https://horace.io/img/perf_intro/multi_operators.png"
      alt="Before Fusion: Illustration taken from Making Deep Learning Go Brrrr From First Principles by Horace He"
      style="width: 300px; max-width: 100%; height: auto;" />
  <div style="font-size: 0.95em; color: #666; margin-top: 4px;">
      Before Fusion: Illustration taken from Making Deep Learning Go Brrrr From First Principles by Horace He
  </div>
  </div>
<p>To save this time, we load the data points once, perform all the compute and then finally send it back to the memory.</p>
  <div style="text-align: center;">
  <img src="https://horace.io/img/perf_intro/operator_fusion.png"
      alt="After Fusion: Illustration taken from Making Deep Learning Go Brrrr From First Principles by Horace He"
      style="width: 300px; max-width: 100%; height: auto;" />
  <div style="font-size: 0.95em; color: #666; margin-top: 4px;">
      After Fusion: Illustration taken from Making Deep Learning Go Brrrr From First Principles by Horace He
  </div>
  </div>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">inductor</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">INFO</span><span style="color:#111">,</span> <span style="color:#111">fusion</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>   <span style="color:#75715e"># Let&#39;s see where the optimization is coming from</span>
</span></span><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">Attention</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">to</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#ae81ff">4096</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span> 
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">)(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">execution_time</span> <span style="color:#f92672">=</span> <span style="color:#111">triton</span><span style="color:#f92672">.</span><span style="color:#111">testing</span><span style="color:#f92672">.</span><span style="color:#111">do_bench</span><span style="color:#111">(</span><span style="color:#00a8c8">lambda</span><span style="color:#111">:</span> <span style="color:#111">model</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time to execute: </span><span style="color:#d88200">{</span><span style="color:#111">execution_time</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span>
</span></span></code></pre></div><p>TorchInductor performs operations fusions in an iterative manner. Once it performs any fusion, it checks again for any further possible fusion.</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">===</span> <span style="color:#111">Fusion</span> <span style="color:#111">Round</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">===</span>
</span></span><span style="display:flex;"><span><span style="color:#111">Candidates</span> <span style="color:#00a8c8">for</span> <span style="color:#111">fusion</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op0&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op1&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op2&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op3&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">SchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op4&#39;</span><span style="color:#111">),</span> <span style="color:#111">Reduction</span><span style="color:#111">([</span><span style="color:#ae81ff">1024</span><span style="color:#111">],</span> <span style="color:#111">sum</span><span style="color:#111">,</span> <span style="color:#111">origins</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#111">sum_1</span><span style="color:#111">,</span> <span style="color:#111">exp</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">SchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op5&#39;</span><span style="color:#111">),</span> <span style="color:#111">Pointwise</span><span style="color:#111">([</span><span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">],</span> <span style="color:#111">origins</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#111">div</span><span style="color:#111">,</span> <span style="color:#111">exp</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op6&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#111">Found</span> <span style="color:#ae81ff">1</span> <span style="color:#111">possible</span> <span style="color:#111">fusion</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">Fusing</span> <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#111">op4</span><span style="color:#960050;background-color:#1e0010">`</span> <span style="color:#00a8c8">with</span> <span style="color:#960050;background-color:#1e0010">`</span><span style="color:#111">op5</span><span style="color:#960050;background-color:#1e0010">`</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#111">Result</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">Fused</span> <span style="color:#ae81ff">7</span> <span style="color:#111">nodes</span> <span style="color:#111">into</span> <span style="color:#ae81ff">6</span> <span style="color:#111">nodes</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">===</span> <span style="color:#111">Fusion</span> <span style="color:#111">Round</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">===</span>
</span></span><span style="display:flex;"><span><span style="color:#111">Candidates</span> <span style="color:#00a8c8">for</span> <span style="color:#111">fusion</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op0&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op1&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op2&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op3&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">FusedSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">op4_op5</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> <span style="color:#111">op4</span><span style="color:#111">:</span> <span style="color:#111">Reduction</span><span style="color:#111">([</span><span style="color:#ae81ff">1024</span><span style="color:#111">],</span> <span style="color:#111">sum</span><span style="color:#111">,</span> <span style="color:#111">origins</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#111">sum_1</span><span style="color:#111">,</span> <span style="color:#111">exp</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">-</span> <span style="color:#111">op5</span><span style="color:#111">:</span> <span style="color:#111">Pointwise</span><span style="color:#111">([</span><span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">],</span> <span style="color:#111">origins</span><span style="color:#f92672">=</span><span style="color:#111">[</span><span style="color:#111">div</span><span style="color:#111">,</span> <span style="color:#111">exp</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">ExternKernelSchedulerNode</span><span style="color:#111">(</span><span style="color:#111">name</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;op6&#39;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#111">Found</span> <span style="color:#ae81ff">0</span> <span style="color:#111">possible</span> <span style="color:#111">fusions</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#111">Nodes</span> <span style="color:#111">remain</span> <span style="color:#111">unchanged</span> <span style="color:#111">(</span><span style="color:#ae81ff">6</span> <span style="color:#960050;background-color:#1e0010">→</span> <span style="color:#ae81ff">6</span><span style="color:#111">)</span>
</span></span></code></pre></div></li>
<li>
<p><strong>Compiled Triton Code (GPUs)</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Kernel definition</span>
</span></span><span style="display:flex;"><span><span style="color:#75af00">@triton_heuristics.pointwise</span><span style="color:#111">(</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">size_hints</span><span style="color:#f92672">=</span><span style="color:#111">{</span><span style="color:#d88200">&#39;x&#39;</span><span style="color:#111">:</span> <span style="color:#ae81ff">16</span><span style="color:#111">},</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">filename</span><span style="color:#f92672">=</span><span style="color:#111">__file__</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">triton_meta</span><span style="color:#f92672">=</span><span style="color:#111">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;signature&#39;</span><span style="color:#111">:</span> <span style="color:#111">{</span><span style="color:#d88200">&#39;in_ptr0&#39;</span><span style="color:#111">:</span> <span style="color:#d88200">&#39;*fp32&#39;</span><span style="color:#111">,</span> <span style="color:#d88200">&#39;out_ptr0&#39;</span><span style="color:#111">:</span> <span style="color:#d88200">&#39;*fp32&#39;</span><span style="color:#111">,</span> <span style="color:#d88200">&#39;xnumel&#39;</span><span style="color:#111">:</span> <span style="color:#d88200">&#39;i32&#39;</span><span style="color:#111">},</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;device&#39;</span><span style="color:#111">:</span> <span style="color:#111">DeviceProperties</span><span style="color:#111">(</span><span style="color:#111">type</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;cuda&#39;</span><span style="color:#111">,</span> <span style="color:#111">index</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#111">multi_processor_count</span><span style="color:#f92672">=</span><span style="color:#ae81ff">40</span><span style="color:#111">,</span> <span style="color:#111">cc</span><span style="color:#f92672">=</span><span style="color:#ae81ff">75</span><span style="color:#111">,</span> <span style="color:#111">major</span><span style="color:#f92672">=</span><span style="color:#ae81ff">7</span><span style="color:#111">,</span> <span style="color:#111">regs_per_multiprocessor</span><span style="color:#f92672">=</span><span style="color:#ae81ff">65536</span><span style="color:#111">,</span> <span style="color:#111">max_threads_per_multi_processor</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#111">warp_size</span><span style="color:#f92672">=</span><span style="color:#ae81ff">32</span><span style="color:#111">),</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;constants&#39;</span><span style="color:#111">:</span> <span style="color:#111">{},</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;configs&#39;</span><span style="color:#111">:</span> <span style="color:#111">[</span><span style="color:#111">AttrsDescriptor</span><span style="color:#f92672">.</span><span style="color:#111">from_dict</span><span style="color:#111">({</span><span style="color:#d88200">&#39;arg_properties&#39;</span><span style="color:#111">:</span> <span style="color:#111">{</span><span style="color:#d88200">&#39;tt.divisibility&#39;</span><span style="color:#111">:</span> <span style="color:#111">(</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">2</span><span style="color:#111">),</span> <span style="color:#d88200">&#39;tt.equal_to&#39;</span><span style="color:#111">:</span> <span style="color:#111">()},</span> <span style="color:#d88200">&#39;cls&#39;</span><span style="color:#111">:</span> <span style="color:#d88200">&#39;AttrsDescriptor&#39;</span><span style="color:#111">})]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">},</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">inductor_meta</span><span style="color:#f92672">=</span><span style="color:#111">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;autotune_hints&#39;</span><span style="color:#111">:</span> <span style="color:#111">set</span><span style="color:#111">(),</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;kernel_name&#39;</span><span style="color:#111">:</span> <span style="color:#d88200">&#39;triton_poi_fused_div_exp_sum_0&#39;</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;mutated_arg_names&#39;</span><span style="color:#111">:</span> <span style="color:#111">[],</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;optimize_mem&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;no_x_dim&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;num_load&#39;</span><span style="color:#111">:</span> <span style="color:#ae81ff">5</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;num_reduction&#39;</span><span style="color:#111">:</span> <span style="color:#ae81ff">0</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;backend_hash&#39;</span><span style="color:#111">:</span> <span style="color:#d88200">&#39;9182018CCD6A4F758231D68D0B1E1E23CEBB32E5D78CB36B65791C4EB96774A2&#39;</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;are_deterministic_algorithms_enabled&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;assert_indirect_indexing&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">True</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;autotune_local_cache&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">True</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;autotune_pointwise&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">True</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;autotune_remote_cache&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">None</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;force_disable_caches&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;dynamic_scale_rblock&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">True</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;max_autotune&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;max_autotune_pointwise&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;min_split_scan_rblock&#39;</span><span style="color:#111">:</span> <span style="color:#ae81ff">256</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;spill_threshold&#39;</span><span style="color:#111">:</span> <span style="color:#ae81ff">16</span><span style="color:#111">,</span>
</span></span><span style="display:flex;"><span>        <span style="color:#d88200">&#39;store_cubin&#39;</span><span style="color:#111">:</span> <span style="color:#00a8c8">False</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">},</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">min_elem_per_thread</span><span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#75af00">@triton.jit</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">def</span> <span style="color:#75af00">triton_poi_fused_div_exp_sum_0</span><span style="color:#111">(</span><span style="color:#111">in_ptr0</span><span style="color:#111">,</span> <span style="color:#111">out_ptr0</span><span style="color:#111">,</span> <span style="color:#111">xnumel</span><span style="color:#111">,</span> <span style="color:#111">XBLOCK</span><span style="color:#111">:</span> <span style="color:#111">tl</span><span style="color:#f92672">.</span><span style="color:#111">constexpr</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Kernel implementation</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">xnumel</span> <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">xoffset</span> <span style="color:#f92672">=</span> <span style="color:#111">tl</span><span style="color:#f92672">.</span><span style="color:#111">program_id</span><span style="color:#111">(</span><span style="color:#ae81ff">0</span><span style="color:#111">)</span> <span style="color:#f92672">*</span> <span style="color:#111">XBLOCK</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">xindex</span> <span style="color:#f92672">=</span> <span style="color:#111">xoffset</span> <span style="color:#f92672">+</span> <span style="color:#111">tl</span><span style="color:#f92672">.</span><span style="color:#111">arange</span><span style="color:#111">(</span><span style="color:#ae81ff">0</span><span style="color:#111">,</span> <span style="color:#111">XBLOCK</span><span style="color:#111">)[:]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">xmask</span> <span style="color:#f92672">=</span> <span style="color:#111">xindex</span> <span style="color:#f92672">&lt;</span> <span style="color:#111">xnumel</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Call function</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">def</span> <span style="color:#75af00">call</span><span style="color:#111">(</span><span style="color:#111">args</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Allocate memory for output tensors</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">primals_1</span><span style="color:#111">,</span> <span style="color:#111">primals_2</span><span style="color:#111">,</span> <span style="color:#111">primals_3</span><span style="color:#111">,</span> <span style="color:#111">primals_4</span> <span style="color:#f92672">=</span> <span style="color:#111">args</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">args</span><span style="color:#f92672">.</span><span style="color:#111">clear</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Call external kernels</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">buf0</span> <span style="color:#f92672">=</span> <span style="color:#111">empty_strided_cuda</span><span style="color:#111">((</span><span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">extern_kernels</span><span style="color:#f92672">.</span><span style="color:#111">mm</span><span style="color:#111">(</span><span style="color:#111">reinterpret_tensor</span><span style="color:#111">(</span><span style="color:#111">primals_2</span><span style="color:#111">,</span> <span style="color:#111">(</span><span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#ae81ff">0</span><span style="color:#111">),</span> <span style="color:#111">reinterpret_tensor</span><span style="color:#111">(</span><span style="color:#111">primals_1</span><span style="color:#111">,</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#ae81ff">0</span><span style="color:#111">),</span> <span style="color:#111">out</span><span style="color:#f92672">=</span><span style="color:#111">buf0</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># ...</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Launch Triton kernel</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">stream0</span> <span style="color:#f92672">=</span> <span style="color:#111">get_raw_stream</span><span style="color:#111">(</span><span style="color:#ae81ff">0</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">triton_poi_fused_div_exp_sum_0</span><span style="color:#f92672">.</span><span style="color:#111">run</span><span style="color:#111">(</span><span style="color:#111">buf3</span><span style="color:#111">,</span> <span style="color:#111">buf4</span><span style="color:#111">,</span> <span style="color:#ae81ff">16</span><span style="color:#111">,</span> <span style="color:#111">grid</span><span style="color:#f92672">=</span><span style="color:#111">grid</span><span style="color:#111">(</span><span style="color:#ae81ff">16</span><span style="color:#111">),</span> <span style="color:#111">stream</span><span style="color:#f92672">=</span><span style="color:#111">stream0</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Return output tensors</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">return</span> <span style="color:#111">(</span><span style="color:#111">buf5</span><span style="color:#111">,</span> <span style="color:#111">reinterpret_tensor</span><span style="color:#111">(</span><span style="color:#111">primals_2</span><span style="color:#111">,</span> <span style="color:#111">(</span><span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#ae81ff">0</span><span style="color:#111">),</span> <span style="color:#111">buf3</span><span style="color:#111">,</span> <span style="color:#111">buf4</span><span style="color:#111">,</span> <span style="color:#111">reinterpret_tensor</span><span style="color:#111">(</span><span style="color:#111">buf2</span><span style="color:#111">,</span> <span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">256</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#ae81ff">0</span><span style="color:#111">),</span> <span style="color:#111">reinterpret_tensor</span><span style="color:#111">(</span><span style="color:#111">buf0</span><span style="color:#111">,</span> <span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">256</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#ae81ff">0</span><span style="color:#111">),</span> <span style="color:#111">reinterpret_tensor</span><span style="color:#111">(</span><span style="color:#111">buf1</span><span style="color:#111">,</span> <span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">256</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#ae81ff">0</span><span style="color:#111">),</span> <span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Benchmark function</span>
</span></span><span style="display:flex;"><span><span style="color:#00a8c8">def</span> <span style="color:#75af00">benchmark_compiled_module</span><span style="color:#111">(</span><span style="color:#111">times</span><span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#111">,</span> <span style="color:#111">repeat</span><span style="color:#f92672">=</span><span style="color:#ae81ff">10</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">from</span> <span style="color:#111">torch._dynamo.testing</span> <span style="color:#f92672">import</span> <span style="color:#111">rand_strided</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">from</span> <span style="color:#111">torch._inductor.utils</span> <span style="color:#f92672">import</span> <span style="color:#111">print_performance</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">primals_1</span> <span style="color:#f92672">=</span> <span style="color:#111">rand_strided</span><span style="color:#111">((</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;cuda:0&#39;</span><span style="color:#111">,</span> <span style="color:#111">dtype</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">primals_2</span> <span style="color:#f92672">=</span> <span style="color:#111">rand_strided</span><span style="color:#111">((</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">256</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;cuda:0&#39;</span><span style="color:#111">,</span> <span style="color:#111">dtype</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">primals_3</span> <span style="color:#f92672">=</span> <span style="color:#111">rand_strided</span><span style="color:#111">((</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;cuda:0&#39;</span><span style="color:#111">,</span> <span style="color:#111">dtype</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">primals_4</span> <span style="color:#f92672">=</span> <span style="color:#111">rand_strided</span><span style="color:#111">((</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">64</span><span style="color:#111">),</span> <span style="color:#111">(</span><span style="color:#ae81ff">64</span><span style="color:#111">,</span> <span style="color:#ae81ff">1</span><span style="color:#111">),</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#d88200">&#39;cuda:0&#39;</span><span style="color:#111">,</span> <span style="color:#111">dtype</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">float32</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>    <span style="color:#111">fn</span> <span style="color:#f92672">=</span> <span style="color:#00a8c8">lambda</span><span style="color:#111">:</span> <span style="color:#111">call</span><span style="color:#111">([</span><span style="color:#111">primals_1</span><span style="color:#111">,</span> <span style="color:#111">primals_2</span><span style="color:#111">,</span> <span style="color:#111">primals_3</span><span style="color:#111">,</span> <span style="color:#111">primals_4</span><span style="color:#111">])</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">return</span> <span style="color:#111">print_performance</span><span style="color:#111">(</span><span style="color:#111">fn</span><span style="color:#111">,</span> <span style="color:#111">times</span><span style="color:#f92672">=</span><span style="color:#111">times</span><span style="color:#111">,</span> <span style="color:#111">repeat</span><span style="color:#f92672">=</span><span style="color:#111">repeat</span><span style="color:#111">)</span>
</span></span></code></pre></div><p>Now if you see here the output code for the torchinductor is nothing but Python. Seems counterintuitive right? We ingest in Python code and burps out Python code but the output Python code is faster.</p>
</li>
</ul>
<h4 id="torchinductor-modes">TorchInductor Modes</h4>
<p>TorchInductor also allows you to select between different types of modes for your use cases.</p>
<table>
<thead>
<tr>
<th>Mode</th>
<th>Purpose</th>
<th>Compilation Time</th>
<th>Runtime Speed</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>default</td>
<td>Balanced compilation and runtime</td>
<td>Moderate</td>
<td>Moderate to high</td>
<td>Good for general use</td>
</tr>
<tr>
<td>reduce-overhead</td>
<td>Reduce Python/kernel launch overhead</td>
<td>Faster</td>
<td>Low latency, especially small batches</td>
<td>Uses CUDA graphs, less flexible suitable for realtime and low latency requirements. Focuses on reducing the CPU to GPU overhead</td>
</tr>
<tr>
<td>max-autotune</td>
<td>Exhaustive autotuning for optimal kernels</td>
<td>Longest</td>
<td>Highest</td>
<td>Uses Triton, CUDA graphs by default for best performance, chooses the best kernel compatible with the hardware</td>
</tr>
<tr>
<td>max-autotune-no-cudagraphs</td>
<td>Autotune w/o CUDA graphs</td>
<td>Long</td>
<td>High</td>
<td>When your hardware don&rsquo;t support CUDA, you want to debug for non deterministic kernel launches or CUDA causing troubles</td>
</tr>
<tr>
<td>fullgraph (flag)</td>
<td>Compile whole model into one graph</td>
<td>Varies (can increase)</td>
<td>Varies</td>
<td>Useful for deployment specially when you understand you model/code can compile, performs fusion aggresively</td>
</tr>
</tbody>
</table>
<h3 id="optimizing-performance">Optimizing Performance</h3>
<p>Now that we have understood what are the components in torch.compile. Let&rsquo;s understand how to debug torch.compile for certain issues:</p>
<ol>
<li>
<p><strong>Recompilations</strong></p>
<p>It may happen that you inference runs might show different (usually higher) latency even after doing torch.compile for some specific inputs. This might be pointing to graph recompilation. Remember we talked about how TorchDynamo uses a Sympy to maintain a log of acceptable shapes. If somehow the guards notices something fishy (like dynamic tensor), they trigger a graph recompilation which takes time.</p>
<p>Let&rsquo;s see it in action:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">Attention</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input with one shape</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x1</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">8</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">st</span> <span style="color:#f92672">=</span> <span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled</span><span style="color:#111">(</span><span style="color:#111">x1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time took to execute I run: </span><span style="color:#d88200">{</span><span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span> <span style="color:#f92672">-</span> <span style="color:#111">st</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input with another shape → triggers recompilation!</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x2</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">st</span> <span style="color:#f92672">=</span> <span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled</span><span style="color:#111">(</span><span style="color:#111">x2</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time took to execute II run: </span><span style="color:#d88200">{</span><span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span> <span style="color:#f92672">-</span> <span style="color:#111">st</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span>
</span></span></code></pre></div><p>Output</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">took</span> <span style="color:#111">to</span> <span style="color:#111">execute</span> <span style="color:#111">I</span> <span style="color:#111">run</span><span style="color:#111">:</span> <span style="color:#ae81ff">0.25</span>
</span></span><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">took</span> <span style="color:#111">to</span> <span style="color:#111">execute</span> <span style="color:#111">II</span> <span style="color:#111">run</span><span style="color:#111">:</span> <span style="color:#ae81ff">0.52</span>
</span></span></code></pre></div><p>The second time is almost 2x - 2.5x higher than previous one. Now let&rsquo;s investigate why this is happening?</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Lets check for the logs for the first run (for recompilation)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">inductor</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">INFO</span><span style="color:#111">,</span> <span style="color:#111">recompiles</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input with shape (8, 1024)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x1</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">8</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">st</span> <span style="color:#f92672">=</span> <span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled</span><span style="color:#111">(</span><span style="color:#111">x1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time took to execute I run: </span><span style="color:#d88200">{</span><span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span> <span style="color:#f92672">-</span> <span style="color:#111">st</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">fx</span> <span style="color:#111">graph</span> <span style="color:#111">cache</span> <span style="color:#111">hit</span> <span style="color:#00a8c8">for</span> <span style="color:#111">key</span> <span style="color:#111">fiaymikawo6wb665p4sxxdl34aqa2dskcjqyqsbposdn3tqywlqm</span>
</span></span><span style="display:flex;"><span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">/</span><span style="color:#ae81ff">0</span><span style="color:#111">]</span> <span style="color:#111">Step</span> <span style="color:#ae81ff">3</span><span style="color:#111">:</span> <span style="color:#111">torchinductor</span> <span style="color:#111">done</span> <span style="color:#111">compiling</span> <span style="color:#111">FORWARDS</span> <span style="color:#111">graph</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">took</span> <span style="color:#111">to</span> <span style="color:#111">execute</span> <span style="color:#111">I</span> <span style="color:#111">run</span><span style="color:#111">:</span> <span style="color:#ae81ff">0.27</span>
</span></span></code></pre></div><p>As expected the input hits the graph and uses it to run the input data which results in faster result(I already compiled the model earlier)</p>
<p>Now, lets consider little tweak in the input shape</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># Lets check for the logs for the first run (for recompilation)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">inductor</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">INFO</span><span style="color:#111">,</span> <span style="color:#111">recompiles</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Input with shape (16, 1024)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x1</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">randn</span><span style="color:#111">(</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">st</span> <span style="color:#f92672">=</span> <span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled</span><span style="color:#111">(</span><span style="color:#111">x1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time took to execute II run: </span><span style="color:#d88200">{</span><span style="color:#111">time</span><span style="color:#f92672">.</span><span style="color:#111">perf_counter</span><span style="color:#111">()</span> <span style="color:#f92672">-</span> <span style="color:#111">st</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200">&#34;</span><span style="color:#111">)</span>
</span></span></code></pre></div><p>Output</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">Recompiling</span> <span style="color:#111">function</span> <span style="color:#111">forward</span> <span style="color:#f92672">in</span> <span style="color:#f92672">/</span><span style="color:#111">tmp</span><span style="color:#f92672">/</span><span style="color:#111">ipython</span><span style="color:#f92672">-</span><span style="color:#111">input</span><span style="color:#f92672">-</span><span style="color:#ae81ff">8</span><span style="color:#f92672">-</span><span style="color:#ae81ff">1568882374.</span><span style="color:#111">py</span><span style="color:#111">:</span><span style="color:#ae81ff">8</span>
</span></span><span style="display:flex;"><span><span style="color:#111">triggered</span> <span style="color:#111">by</span> <span style="color:#111">the</span> <span style="color:#111">following</span> <span style="color:#111">guard</span> <span style="color:#111">failure</span><span style="color:#111">(</span><span style="color:#111">s</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">-</span> <span style="color:#ae81ff">0</span><span style="color:#f92672">/</span><span style="color:#ae81ff">0</span><span style="color:#111">:</span> <span style="color:#111">tensor</span> <span style="color:#d88200">&#39;L[&#39;</span><span style="color:#111">x</span><span style="color:#d88200">&#39;]&#39;</span> <span style="color:#111">size</span> <span style="color:#111">mismatch</span> <span style="color:#111">at</span> <span style="color:#111">index</span> <span style="color:#ae81ff">0.</span> <span style="color:#111">expected</span> <span style="color:#ae81ff">8</span><span style="color:#111">,</span> <span style="color:#111">actual</span> <span style="color:#ae81ff">16</span>
</span></span><span style="display:flex;"><span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">/</span><span style="color:#ae81ff">1</span><span style="color:#111">]</span> <span style="color:#111">fx</span> <span style="color:#111">graph</span> <span style="color:#111">cache</span> <span style="color:#111">hit</span> <span style="color:#00a8c8">for</span> <span style="color:#111">key</span> <span style="color:#111">fk4po6bondjbymxorlxrxr2yh6ksrt4ktp7s4rd6ozzeyqtytm7p</span>
</span></span><span style="display:flex;"><span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">/</span><span style="color:#ae81ff">1</span><span style="color:#111">]</span> <span style="color:#111">Step</span> <span style="color:#ae81ff">3</span><span style="color:#111">:</span> <span style="color:#111">torchinductor</span> <span style="color:#111">done</span> <span style="color:#111">compiling</span> <span style="color:#111">FORWARDS</span> <span style="color:#111">graph</span> <span style="color:#ae81ff">13</span> <span style="color:#111">fx</span> <span style="color:#111">graph</span> <span style="color:#111">cache</span> <span style="color:#111">hit</span> <span style="color:#00a8c8">for</span> <span style="color:#111">key</span> <span style="color:#111">fttbvdnusdbfieyg4rk7mtwujffoewhlpg4mevcicwmsvxcy5vzv</span>
</span></span><span style="display:flex;"><span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#f92672">/</span><span style="color:#ae81ff">1</span><span style="color:#111">]</span> <span style="color:#111">Step</span> <span style="color:#ae81ff">3</span><span style="color:#111">:</span> <span style="color:#111">torchinductor</span> <span style="color:#111">done</span> <span style="color:#111">compiling</span> <span style="color:#111">BACKWARDS</span> <span style="color:#111">graph</span> <span style="color:#ae81ff">13</span>
</span></span><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">took</span> <span style="color:#111">to</span> <span style="color:#111">execute</span> <span style="color:#111">I</span> <span style="color:#111">run</span><span style="color:#111">:</span> <span style="color:#ae81ff">0.40</span>
</span></span></code></pre></div><p>Notice the recompilation got triggered because the graph was expecting a shape of <code>[8, 1024]</code> and not <code>[16, 1024]</code> which resulted in higher time to execution.</p>
</li>
<li>
<p>Compilation Mode
We talked about different modes of compilation which can be suited different according to the use cases. Let&rsquo;s see what fits best for our case.</p>
<p><em>Note - I have picked most widely popular method that works best for most cases.</em></p>
</li>
</ol>
<ul>
<li>
<p>Default Mode</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">inductor</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">ERROR</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">Attention</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">to</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#ae81ff">4096</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>   
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">)(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">execution_time</span> <span style="color:#f92672">=</span> <span style="color:#111">triton</span><span style="color:#f92672">.</span><span style="color:#111">testing</span><span style="color:#f92672">.</span><span style="color:#111">do_bench</span><span style="color:#111">(</span><span style="color:#00a8c8">lambda</span><span style="color:#111">:</span> <span style="color:#111">model</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time to execute: </span><span style="color:#d88200">{</span><span style="color:#111">execution_time</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200"> ms&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span>          <span style="color:#75715e"># Resets the graph captured and clears the cache</span>
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">to</span> <span style="color:#111">execute</span><span style="color:#111">:</span> <span style="color:#ae81ff">28.55</span> <span style="color:#111">ms</span>
</span></span></code></pre></div><p>Not bad for default, now lets see for other modes.</p>
</li>
<li>
<p>Max-autotune</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">inductor</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">ERROR</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">Attention</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">to</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#ae81ff">4096</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>   
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">,</span> <span style="color:#111">mode</span><span style="color:#f92672">=</span><span style="color:#d88200">&#34;max-autotune&#34;</span><span style="color:#111">)(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">execution_time</span> <span style="color:#f92672">=</span> <span style="color:#111">triton</span><span style="color:#f92672">.</span><span style="color:#111">testing</span><span style="color:#f92672">.</span><span style="color:#111">do_bench</span><span style="color:#111">(</span><span style="color:#00a8c8">lambda</span><span style="color:#111">:</span> <span style="color:#111">model</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time to execute: </span><span style="color:#d88200">{</span><span style="color:#111">execution_time</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200"> ms&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span>          <span style="color:#75715e"># Resets the graph captured and clears the cache</span>
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">to</span> <span style="color:#111">execute</span><span style="color:#111">:</span> <span style="color:#ae81ff">28.55</span> <span style="color:#111">ms</span>
</span></span></code></pre></div></li>
<li>
<p>reduce-overhead</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">inductor</span><span style="color:#f92672">=</span><span style="color:#111">logging</span><span style="color:#f92672">.</span><span style="color:#111">ERROR</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">Attention</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">4096</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">to</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">,</span> <span style="color:#ae81ff">4096</span><span style="color:#111">,</span> <span style="color:#111">device</span><span style="color:#f92672">=</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">device</span><span style="color:#111">(</span><span style="color:#d88200">&#34;cuda&#34;</span><span style="color:#111">))</span>   
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">,</span> <span style="color:#111">mode</span><span style="color:#f92672">=</span><span style="color:#d88200">&#34;reduce-overhead&#34;</span><span style="color:#111">)(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">execution_time</span> <span style="color:#f92672">=</span> <span style="color:#111">triton</span><span style="color:#f92672">.</span><span style="color:#111">testing</span><span style="color:#f92672">.</span><span style="color:#111">do_bench</span><span style="color:#111">(</span><span style="color:#00a8c8">lambda</span><span style="color:#111">:</span> <span style="color:#111">model</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">))</span>
</span></span><span style="display:flex;"><span><span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">f</span><span style="color:#d88200">&#34;Time to execute: </span><span style="color:#d88200">{</span><span style="color:#111">execution_time</span><span style="color:#d88200">:</span><span style="color:#d88200">.2f</span><span style="color:#d88200">}</span><span style="color:#d88200"> ms&#34;</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span>          <span style="color:#75715e"># Resets the graph captured and clears the cache</span>
</span></span></code></pre></div><p>Output:</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">Time</span> <span style="color:#111">to</span> <span style="color:#111">execute</span><span style="color:#111">:</span> <span style="color:#ae81ff">28.55</span> <span style="color:#111">ms</span>
</span></span></code></pre></div></li>
</ul>
<h2 id="compilation-failures">Compilation Failures</h2>
<p>There can be scenarious where you might not able to compile your code. There are reasons for that and we need to ensure we don&rsquo;t include them in our code.</p>
<ul>
<li>
<p><strong>Control Flow</strong>
Python control flow decisions depend on runtime tensor values PyTorch doesn&rsquo;t evaluate the tensor value. It builds a symbolic graph, given now you have a conditional output. PyTorch gets confused which path to pick since its not determined and depends on the input data.</p>
</li>
<li>
<p><strong>Printing and Logging</strong>
Adding logging or priting statements also makes it difficult for the PyTorch to compile the python code.</p>
</li>
<li>
<p><strong>Non-Tensor</strong>
Since you are working with PyTorch, it expects to handle only Tensor values. Any non-tensor value such as list, tuple might also lead to graph breaking</p>
</li>
<li>
<p><strong>Modifying data on runtime</strong>
Modifying any data during the runtime also results in graph breaking.</p>
</li>
<li>
<p><strong>Custom operation or library kernel</strong>
Any custom operation which is not covered by PyTorch or any library which is not ready for torch.compile might also result in failures.</p>
</li>
</ul>
<p>Let&rsquo;s take an example to see how it actually looks like in action?</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a8c8">class</span> <span style="color:#75af00">BrokenAttention</span><span style="color:#111">(</span><span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Module</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#111">__init__</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#f92672">=</span><span style="color:#ae81ff">1024</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">super</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">__init__</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_q</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">bias</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_k</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">bias</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_v</span> <span style="color:#f92672">=</span> <span style="color:#111">nn</span><span style="color:#f92672">.</span><span style="color:#111">Linear</span><span style="color:#111">(</span><span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">dim</span><span style="color:#111">,</span> <span style="color:#111">bias</span><span style="color:#f92672">=</span><span style="color:#00a8c8">False</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a8c8">def</span> <span style="color:#75af00">forward</span><span style="color:#111">(</span><span style="color:#111">self</span><span style="color:#111">,</span> <span style="color:#111">x</span><span style="color:#111">:</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">Tensor</span><span style="color:#111">):</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Illegal: Python side-effect + int computation based on tensor</span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">if</span> <span style="color:#111">x</span><span style="color:#f92672">.</span><span style="color:#111">shape</span><span style="color:#111">[</span><span style="color:#ae81ff">0</span><span style="color:#111">]</span> <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">8</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span>            <span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">&#34;Batch too big!&#34;</span><span style="color:#111">)</span>  <span style="color:#75715e"># Graph break here!</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">q</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_q</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">k</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_k</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">v</span> <span style="color:#f92672">=</span> <span style="color:#111">self</span><span style="color:#f92672">.</span><span style="color:#111">W_v</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">out</span> <span style="color:#f92672">=</span> <span style="color:#111">q</span> <span style="color:#f92672">@</span> <span style="color:#111">k</span><span style="color:#f92672">.</span><span style="color:#111">transpose</span><span style="color:#111">(</span><span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#111">,</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#111">out</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">exp</span><span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">)</span> <span style="color:#f92672">/</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">sum</span><span style="color:#111">(</span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">exp</span><span style="color:#111">(</span><span style="color:#111">out</span><span style="color:#111">),</span> <span style="color:#111">dim</span><span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span><span style="color:#111">,</span> <span style="color:#111">keepdim</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span>        <span style="color:#00a8c8">return</span> <span style="color:#111">out</span> <span style="color:#f92672">@</span> <span style="color:#111">v</span>
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#111">logging_api</span><span style="color:#f92672">.</span><span style="color:#111">set_logs</span><span style="color:#111">(</span><span style="color:#111">graph_breaks</span><span style="color:#f92672">=</span><span style="color:#00a8c8">True</span><span style="color:#111">)</span> <span style="color:#75715e"># Try disabling this.</span>
</span></span><span style="display:flex;"><span><span style="color:#111">model</span> <span style="color:#f92672">=</span> <span style="color:#111">BrokenAttention</span><span style="color:#111">()</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">x</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">rand</span><span style="color:#111">(</span><span style="color:#ae81ff">16</span><span style="color:#111">,</span> <span style="color:#ae81ff">4</span><span style="color:#111">,</span> <span style="color:#ae81ff">1024</span><span style="color:#111">)</span><span style="color:#f92672">.</span><span style="color:#111">cuda</span><span style="color:#111">()</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled_model</span> <span style="color:#f92672">=</span> <span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compile</span><span style="color:#111">(</span><span style="color:#111">model</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">compiled_model</span><span style="color:#111">(</span><span style="color:#111">x</span><span style="color:#111">)</span>
</span></span><span style="display:flex;"><span><span style="color:#111">torch</span><span style="color:#f92672">.</span><span style="color:#111">compiler</span><span style="color:#f92672">.</span><span style="color:#111">reset</span><span style="color:#111">()</span>
</span></span></code></pre></div><p>Output</p>
<div class="highlight"><pre tabindex="0" style="color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span> <span style="color:#111">Graph</span> <span style="color:#00a8c8">break</span> <span style="color:#f92672">in</span> <span style="color:#111">user</span> <span style="color:#111">code</span> <span style="color:#111">at</span> <span style="color:#f92672">/</span><span style="color:#111">tmp</span><span style="color:#f92672">/</span><span style="color:#111">ipython</span><span style="color:#f92672">-</span><span style="color:#111">input</span><span style="color:#f92672">-</span><span style="color:#ae81ff">31</span><span style="color:#f92672">-</span><span style="color:#ae81ff">1969035068.</span><span style="color:#111">py</span><span style="color:#111">:</span><span style="color:#ae81ff">11</span>
</span></span><span style="display:flex;"><span> <span style="color:#111">Reason</span><span style="color:#111">:</span> <span style="color:#111">Unsupported</span><span style="color:#111">:</span> <span style="color:#111">builtin</span><span style="color:#111">:</span> <span style="color:#111">print</span> <span style="color:#111">[</span><span style="color:#f92672">&lt;</span><span style="color:#00a8c8">class</span> <span style="color:#960050;background-color:#1e0010">&#39;</span><span style="color:#75af00">torch</span><span style="color:#f92672">.</span><span style="color:#111">_dynamo</span><span style="color:#f92672">.</span><span style="color:#111">variables</span><span style="color:#f92672">.</span><span style="color:#111">constant</span><span style="color:#f92672">.</span><span style="color:#111">ConstantVariable</span><span style="color:#d88200">&#39;&gt;] False</span>
</span></span><span style="display:flex;"><span> <span style="color:#111">User</span> <span style="color:#111">code</span> <span style="color:#111">traceback</span><span style="color:#111">:</span>
</span></span><span style="display:flex;"><span>   <span style="color:#111">File</span> <span style="color:#d88200">&#34;/tmp/ipython-input-31-1969035068.py&#34;</span><span style="color:#111">,</span> <span style="color:#111">line</span> <span style="color:#ae81ff">11</span><span style="color:#111">,</span> <span style="color:#f92672">in</span> <span style="color:#111">forward</span>
</span></span><span style="display:flex;"><span>     <span style="color:#111">print</span><span style="color:#111">(</span><span style="color:#d88200">&#34;Batch too big!&#34;</span><span style="color:#111">)</span>  <span style="color:#75715e"># Graph break here!</span>
</span></span></code></pre></div><h2 id="bonus">Bonus</h2>
<p>All the codes and logs are available in this <a href="https://colab.research.google.com/drive/13Awttk7WxJ9nUGpviHUTB1RIF8l0DsH8?usp=sharing">notebook</a>. Feel free to play around and unwrap the layers of pytorch.compile.</p>
<p>Happy Learning!</p>
<p><em>The best environment variable for debugging is</em>
<code>TORCH_COMPILE_DEBUG=1</code></p>
<h4 id="references">References</h4>
<ol>
<li><a href="https://www.youtube.com/live/1FSBurHpH_Q?si=NMWkZYZx1FaNQxYs">PyTorch 2.0 Live Q&amp;A Series: PT2 Profiling and Debugging</a></li>
<li><a href="https://blog.ezyang.com/2024/11/ways-to-use-torch-compile/">Ezyang&rsquo;s ways to use torch compile</a></li>
<li><a href="https://docs.google.com/document/d/1y5CRfMLdwEoF1nTk9q8qEu1mgMUuUtvhklPKJ2emLU8/edit?tab=t.0">torch.compile, the missing manual</a></li>
<li><a href="https://docs.pytorch.org/docs/stable/logging.html">PyTorch Logging</a></li>
<li><a href="https://zdevito.github.io/2022/08/16/memory-snapshots.html">Debugging PyTorch Memory with Snapshot</a></li>
<li><a href="https://docs.pytorch.org/docs/stable/torch.compiler.html">pytorch.compile docs</a></li>
<li><a href="https://horace.io/brrr_intro.html">Making GPUs go brr by Horace He</a></li>
<li><a href="https://dev.to/minwook/pytorch-compile-vs-export-omc">PyTorch Compile vs Export</a>
9 <a href="https://www.adamcasson.com/posts/torch-compile-vit">How does torch.compile speed up a transformer</a></li>
</ol>

      
      <div class="post-date">
        <span class="g time">July 22, 2025 </span> &#8729;
         
      </div>
      
    </section>
    
    
    
  </div>
</main>
</body>
</html>
